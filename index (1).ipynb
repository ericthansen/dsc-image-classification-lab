{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Now that you have a working knowledge of CNNs and have practiced implementing associated techniques in Keras, its time to put all of those skills together. In this lab, you'll work to complete a [Kaggle competition](https://www.kaggle.com/c/dog-breed-identification) on classifying dog breeds.\n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this lab you will: \n",
    "\n",
    "- Compare and apply multiple techniques for tuning a model using data augmentation and pretrained models  \n",
    "\n",
    "## Download and Load the Data\n",
    "\n",
    "Start by downloading the data locally and loading it into a Pandas DataFrame. Be forewarned that this dataset is fairly large and it is advisable to close other memory intensive applications.\n",
    "\n",
    "The data can be found [here](https://www.kaggle.com/c/dog-breed-identification/data).\n",
    "\n",
    "It's easiest if you download the data into this directory on your local computer. From there, be sure to uncompress the folder and subfolders. If you download the data elsewhere, be sure to modify the file path when importing the file below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No code per se, but download and decompress the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Now that you've downloaded the data, its time to prepare it for some model building! You'll notice that the current structure provided is not the same as our lovely preprocessed folders that you've been given to date. Instead, you have one large training folder with images and a csv file with labels associated with each of these file types. \n",
    "\n",
    "Use this to create a directory substructure for a train-validation-test split as we have done previously. Also recall that you'll also want to use one-hot encoding as you are now presented with a multi-class problem as opposed to simple binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000bec180eb18c7604dcecc8fe0dba07</td>\n",
       "      <td>boston_bull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001513dfcb2ffafc82cccf4d8bbaba97</td>\n",
       "      <td>dingo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001cdf01b096e06d78e9e5112d419397</td>\n",
       "      <td>pekinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00214f311d5d2247d5dfe4fe24b2303d</td>\n",
       "      <td>bluetick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0021f9ceb3235effd7fcde7f7538ed62</td>\n",
       "      <td>golden_retriever</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id             breed\n",
       "0  000bec180eb18c7604dcecc8fe0dba07       boston_bull\n",
       "1  001513dfcb2ffafc82cccf4d8bbaba97             dingo\n",
       "2  001cdf01b096e06d78e9e5112d419397          pekinese\n",
       "3  00214f311d5d2247d5dfe4fe24b2303d          bluetick\n",
       "4  0021f9ceb3235effd7fcde7f7538ed62  golden_retriever"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here; open the labels.csv file stored in the zip file\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('labels.csv')\n",
    "df.head()\n",
    "##note, this file wasn't available in initial state, but had to be uploaded to cloud.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'dog_breeds/train/': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "ls dog_breeds/train/ | head -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline_CNN_dog_subset_run2.h5  LICENSE.md\r\n",
      "Baseline_CNN.h5                  multiclass_cnfmatx.png\r\n",
      "CONTRIBUTING.md                  README.md\r\n",
      "\u001b[0m\u001b[01;34mdata_org_subset\u001b[0m/                 vgg19_3breeds_4epochs.h5\r\n",
      "history_vgg19_10epochs.pickle    vgg19_FE_AUG_10epochs.h5\r\n",
      "index.ipynb                      vgg19_FE_AUG_15epochs.h5\r\n",
      "labels.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In order to input the data into our standard pipeline, you'll need to organize the image files into a nested folder structure. At the top level will be a folder for the training data, a folder for the validation data, and a folder for the test data. Within these top directory folders, you'll then need to create a folder for each of the categorical classes (in this case, dog breeds). Finally, within these category folders you'll then place each of the associated image files. To save time, do this for just 3 of the dog breeds such as `'boston_bull'`, `'toy_poodle'`, and `'scottish_deerhound'`.\n",
    "\n",
    "You're nested file structure should look like this:\n",
    "* train\n",
    "    * category_1\n",
    "    * category_2\n",
    "    * category_3\n",
    "    ...\n",
    "* val\n",
    "    * category_1\n",
    "    * category_2\n",
    "    * category_3\n",
    "    ...\n",
    "* test \n",
    "    * category_1\n",
    "    * category_2\n",
    "    * category_3\n",
    "    ...  \n",
    "\n",
    "> **Hint**: To do this, you can use the `os` module which will you can use to execute many common bash commands straight from your python interpreter. For example, here's how you could make a new folder: \n",
    "\n",
    "```python\n",
    "import os\n",
    "os.mkdir('New_Folder_Name')\n",
    "```\n",
    "Start by creating top level folders for the train, validation, and test sets. Then, use your pandas DataFrame to split the example images for each breed of dog into a 80% train set, and 10% validation and test sets. Use `os.path.join()` with the information from the DataFrame to construct the relevant file path. With this, place the relevant images using the `shutil.copy()` into the appropriate directory. \n",
    "\n",
    ">> **Note**: It is worthwhile to try this exercise on your own, but you can also use the images stored under the `'data_org_subset/'` folder of this repository, in which the Kaggle dataset has already been subset and preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique breeds: 120\n",
      "scottish_deerhound      126\n",
      "maltese_dog             117\n",
      "afghan_hound            116\n",
      "entlebucher             115\n",
      "bernese_mountain_dog    114\n",
      "shih-tzu                112\n",
      "great_pyrenees          111\n",
      "pomeranian              111\n",
      "basenji                 110\n",
      "samoyed                 109\n",
      "Name: breed, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique breeds:', df.breed.nunique())\n",
    "print(df.breed.value_counts()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory data_org_subset/ already created\n",
      "directory data_org_subset/train already created\n",
      "directory data_org_subset/val already created\n",
      "directory data_org_subset/test already created\n",
      "Moving boston_bull pictures.\n",
      "directory data_org_subset/train/boston_bull already created\n",
      "directory data_org_subset/val/boston_bull already created\n",
      "directory data_org_subset/test/boston_bull already created\n",
      "Split 87 imgs into 69 train, 9 val, and 9 test examples.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dog_breeds/train/8befc822b56b744d72872428a0ef4851.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_66/4128644500.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0morigin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mdestination\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_root_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdir_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbreed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0;31m# macOS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_HAS_FCOPYFILE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dog_breeds/train/8befc822b56b744d72872428a0ef4851.jpg'"
     ]
    }
   ],
   "source": [
    "# Your code here; transform the image files and then load them into Keras as tensors \n",
    "# (be sure to perform a train-val-test split)\n",
    "import numpy as np\n",
    "import os, shutil\n",
    "\n",
    "old_dir = 'dog_breeds/train/'\n",
    "\n",
    "new_root_dir = 'data_org_subset/'\n",
    "# Because this cell has already been run and this directory now exists, running this cell again will throw an error\n",
    "try:\n",
    "    os.mkdir(new_root_dir) \n",
    "except:\n",
    "    print('directory {} already created'.format(new_root_dir))\n",
    "\n",
    "dir_names = ['train', 'val', 'test']\n",
    "for d in dir_names:\n",
    "    new_dir = os.path.join(new_root_dir, d)\n",
    "    try:\n",
    "        os.mkdir(new_dir)\n",
    "    except:\n",
    "        print('directory {} already created'.format(new_dir))\n",
    "    \n",
    "for breed in ['boston_bull', 'toy_poodle', 'scottish_deerhound']:\n",
    "    print('Moving {} pictures.'.format(breed))\n",
    "    # Create sub_directories\n",
    "    for d in dir_names:\n",
    "        new_dir = os.path.join(new_root_dir, d, breed)\n",
    "        try:\n",
    "            os.mkdir(new_dir)\n",
    "        except:\n",
    "            print('directory {} already created'.format(new_dir))\n",
    "    # Subset dataframe into train, validate and split sets\n",
    "    # Split is performed here to ensure maintain class distributions.\n",
    "    temp = df[df.breed == breed]\n",
    "    train, validate, test = np.split(temp.sample(frac=1), [int(.8*len(temp)), int(.9*len(temp))])\n",
    "    print('Split {} imgs into {} train, {} val, and {} test examples.'.format(len(temp),\n",
    "                                                                              len(train),\n",
    "                                                                              len(validate),\n",
    "                                                                              len(test)))\n",
    "    for i, temp in enumerate([train, validate, test]):\n",
    "        for row in temp.index:\n",
    "            filename = temp['id'][row] + '.jpg'\n",
    "            origin = os.path.join(old_dir + filename)\n",
    "            destination = os.path.join(new_root_dir + dir_names[i] + '/' + breed + '/' + filename)\n",
    "            shutil.copy(origin, destination)\n",
    "            \n",
    "            \n",
    "###similarly, note that all the necessary subsetting has been done in data_org_subset, this cell would only \n",
    "##be necessary for the local version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-12 22:20:18.730845: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-12 22:20:18.730879: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 233 images belonging to 3 classes.\n",
      "Found 30 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_dir = '{}train'.format(new_root_dir)\n",
    "\n",
    "validation_dir = '{}val/'.format(new_root_dir)\n",
    "test_dir = '{}test/'.format(new_root_dir)\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(validation_dir, \n",
    "                                                        target_size=(150, 150), \n",
    "                                                        batch_size=20, \n",
    "                                                        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boston_bull': 0, 'scottish_deerhound': 1, 'toy_poodle': 2}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Build a Baseline CNN\n",
    "\n",
    "This is an optional step. Adapting a pretrained model will produce better results, but it may be interesting to create a CNN from scratch as a baseline. If you wish to, do so here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a baseline CNN model\n",
    "import datetime\n",
    "\n",
    "original_start = datetime.datetime.now()\n",
    "start = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n",
      "/tmp/ipykernel_66/722086529.py:32: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(train_generator,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 12/100 [==>...........................] - ETA: 14s - loss: 1.0964 - acc: 0.3906WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1000 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 50 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 3s 23ms/step - loss: 1.0964 - acc: 0.3906 - val_loss: 1.0790 - val_acc: 0.4333\n",
      "Training took a total of 0:00:03.724982\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "#import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "import datetime\n",
    "\n",
    "original_start = datetime.datetime.now()\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                              steps_per_epoch=100,\n",
    "                              epochs=10,\n",
    "                              validation_data=validation_generator,\n",
    "                              validation_steps=50)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print('Training took a total of {}'.format(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Baseline_CNN_dog_subset_run2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "                                                  target_size=(150, 150),\n",
    "                                                  batch_size=20,\n",
    "                                                  class_mode='categorical',\n",
    "                                                  shuffle=False)\n",
    "\n",
    "test_loss, test_acc = model.evaluate_generator(test_generator, steps=54)\n",
    "y_hat_test = model.predict_generator(test_generator, steps=54)\n",
    "print('Generated {} predictions'.format(len(y_hat_test)))\n",
    "print('test acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Pretrained CNN\n",
    "\n",
    "## Feature Engineering with the Pretrained Model\n",
    "\n",
    "As you may well have guessed, adapting a pretrained model will undoubtedly produce better results then a fresh CNN due to the limited size of training data. Import a pretrained model such as VGG-19 to use a convolutional base. Use this to transform the dataset into a rich feature space and add a few fully connected layers on top of the pretrained layers to build a classification model. (Be sure to leave the pretrained model frozen!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg19 False\n",
      "flatten_2 True\n",
      "dense_10 True\n",
      "dense_11 True\n",
      "dense_12 True\n",
      "dense_13 True\n",
      "dense_14 True\n",
      "10\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg19 (Functional)          (None, 7, 7, 512)         20024384  \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                1605696   \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,704,707\n",
      "Trainable params: 1,680,323\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Your code here; add fully connected layers on top of the convolutional base\n",
    "# Your code here; add fully connected layers on top of the convolutional base\n",
    "# from keras.preprocessing.image import ImageDataGenerator, array_to_img\n",
    "\n",
    "# Initialize Base\n",
    "from tensorflow.keras.applications import VGG19\n",
    "cnn_base = VGG19(weights='imagenet',\n",
    "                 include_top=False,\n",
    "                 input_shape=(240, 240, 3))\n",
    "\n",
    "# Define Model Architecture\n",
    "model = models.Sequential()\n",
    "model.add(cnn_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "cnn_base.trainable = False\n",
    "\n",
    "# You can check whether a layer is trainable (or alter its setting) through the layer.trainable attribute\n",
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable)\n",
    "    \n",
    "# Similarly, we can check how many trainable weights are in the model \n",
    "print(len(model.trainable_weights))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 233 images belonging to 3 classes.\n",
      "Found 30 images belonging to 3 classes.\n",
      "Found 30 images belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n",
      "/tmp/ipykernel_66/32214809.py:63: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(train_generator,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "8/8 [==============================] - ETA: 0s - loss: 1.0843 - acc: 0.4444WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10 batches). You may need to use the repeat() function when building your dataset.\n",
      "8/8 [==============================] - 13s 2s/step - loss: 1.0843 - acc: 0.4444 - val_loss: 1.0508 - val_acc: 0.4333\n",
      "Epoch 2/4\n",
      "8/8 [==============================] - 10s 1s/step - loss: 1.0585 - acc: 0.4314\n",
      "Epoch 3/4\n",
      "8/8 [==============================] - 10s 1s/step - loss: 1.0516 - acc: 0.4248\n",
      "Epoch 4/4\n",
      "8/8 [==============================] - 10s 1s/step - loss: 1.0287 - acc: 0.4379\n",
      "Training took a total of 0:00:43.573942\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing/Problem Setup\n",
    "new_root_dir = 'data_org_subset/'\n",
    "\n",
    "train_dir = '{}train'.format(new_root_dir)\n",
    "validation_dir = '{}val/'.format(new_root_dir)\n",
    "test_dir = '{}test/'.format(new_root_dir)\n",
    "\n",
    "original_start = datetime.datetime.now()\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "# Define Initial Parameters (same as previous code block)\n",
    "datagen = ImageDataGenerator(rescale=1./255) \n",
    "batch_size = 10\n",
    "\n",
    "# Get all the data in the directory split/train (542 images), and reshape them\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                  rotation_range=40,\n",
    "                                  width_shift_range=0.2,\n",
    "                                  height_shift_range=0.2,\n",
    "                                  shear_range=0.2,\n",
    "                                  zoom_range=0.2,\n",
    "                                  horizontal_flip=True,\n",
    "                                  fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, \n",
    "                                                    target_size=(240, 240), \n",
    "                                                    batch_size= 20,\n",
    "                                                    class_mode='categorical') \n",
    "\n",
    "# Get all the data in the directory split/validation (200 images), and reshape them\n",
    "val_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(validation_dir, \n",
    "                                                                       target_size=(240, 240), \n",
    "                                                                       batch_size=20,\n",
    "                                                                       class_mode='categorical')\n",
    "\n",
    "# Get all the data in the directory split/test (180 images), and reshape them\n",
    "test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(test_dir, \n",
    "                                                                        target_size=(240, 240), \n",
    "                                                                        batch_size=180,\n",
    "                                                                        class_mode='categorical',\n",
    "                                                                        shuffle=False)\n",
    "\n",
    "        \n",
    "test_images, test_labels = next(test_generator)\n",
    "\n",
    "\n",
    "# Compilation\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Fitting the Model\n",
    "history = model.fit_generator(train_generator,\n",
    "                              steps_per_epoch=8,\n",
    "                              epochs=4,\n",
    "                              validation_data=val_generator,\n",
    "                              validation_steps=10)\n",
    "\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print('Training took a total of {}'.format(elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize History\n",
    "\n",
    "Now fit the model and visualize the training and validation accuracy/loss functions over successive epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (4,) and (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_66/4111500310.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training acc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation acc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training and validation accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3017\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3018\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3019\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   3020\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \"\"\"\n\u001b[1;32m   1604\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    502\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (4,) and (1,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your code here; visualize the training / validation history associated with fitting the model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()# Your code here; visualize the training / validation history associated with fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save('vgg19_3breeds_4epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-13 03:37:15.689429: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://95179652-8dcf-4997-918d-d7ebcdf9b1bb/assets\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('history_vgg19__3breeds_4epochs.pickle', 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available\n",
    "    pickle.dump(history, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Evaluation\n",
    "\n",
    "Now that you've trained and validated the model, perform a final evaluation of the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30 images belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66/190751594.py:8: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  test_loss, test_acc = model.evaluate_generator(test_generator, steps=54)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 54 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66/190751594.py:9: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  y_hat_test = model.predict_generator(test_generator, steps=54)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 54 batches). You may need to use the repeat() function when building your dataset.\n",
      "Generated 30 predictions\n",
      "test acc: 0.4333333373069763\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "                                                  target_size=(240, 240),\n",
    "                                                  batch_size=20,\n",
    "                                                  class_mode='categorical',\n",
    "                                                  shuffle=False)\n",
    "\n",
    "test_loss, test_acc = model.evaluate_generator(test_generator, steps=54)\n",
    "y_hat_test = model.predict_generator(test_generator, steps=54)\n",
    "print('Generated {} predictions'.format(len(y_hat_test)))\n",
    "print('test acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! In this lab, you brought all of your prior deep learning skills together from preprocessing including one-hot encoding, to adapting a pretrained model. There are always ongoing advancements in CNN architectures and best practices, but you have a solid foundation and understanding at this point."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
